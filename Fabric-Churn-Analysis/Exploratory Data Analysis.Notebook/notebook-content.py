# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "a7e509e0-e547-489e-b9df-2956ca626043",
# META       "default_lakehouse_name": "Silver_Shortcut_LH",
# META       "default_lakehouse_workspace_id": "3b76ac3a-7116-4afb-ae8c-d85acd950cfd",
# META       "known_lakehouses": [
# META         {
# META           "id": "a7e509e0-e547-489e-b9df-2956ca626043"
# META         }
# META       ]
# META     }
# META   }
# META }

# MARKDOWN ********************

# # Perform Exploratory Data Analysis using Fabric Notebooks
# This is a slightly consolidated approach to https://learn.microsoft.com/en-us/fabric/data-science/customer-churn focusing on exploring the churn data using a Fabric notebook.

# CELL ********************

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
from matplotlib import rc, rcParams
import numpy as np
import pandas as pd
import itertools

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark",
# META   "editable": true
# META }

# MARKDOWN ********************

# ## Display Records from Churn Silver Lakehouse
# Use spark to display 10k records.

# CELL ********************

df = spark.sql("SELECT * FROM Silver_Shortcut_LH.silver.silver_churn LIMIT 10000")
display(df)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Transform Data Using Data Wrangler
# By using Data Wrangler, simplify data transformation, and the code is automatically inputted below.

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

def clean_data(df):
    # Drop duplicate rows in columns: 'customer_id', 'surname'
    df = df.dropDuplicates(['customer_id', 'surname'])
    # Drop columns: ['customer_id','surname', 'ingestion_date]
    df = df.drop('customer_id','surname', 'ingestion_date')
    return df

df_clean = clean_data(df)
display(df_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Name and display categorical and numeric columns

# CELL ********************

# Determine the dependent (target) attribute
dependent_variable_name = "is_churned"
print(dependent_variable_name)

# For PySpark DataFrames, use .dtypes and .select() for schema info and .distinct().count() for nunique
from pyspark.sql.types import StringType

# Determine the categorical attributes
categorical_variables = [
    col for col, dtype in df_clean.dtypes
    if (dtype == "string" or df_clean.select(col).distinct().count() <= 5)
    and col != "is_churned"
]
print(categorical_variables)

# Determine the numerical attributes
numeric_variables = [
    col for col, dtype in df_clean.dtypes
    if dtype != "string"
    and df_clean.select(col).distinct().count() > 5
]
print(numeric_variables)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

import matplotlib.pyplot as plt
import seaborn as sns

# If using PySpark DataFrame, convert to Pandas first
df_num_cols = df_clean.select(numeric_variables).toPandas()

sns.set(font_scale=0.7)
fig, axes = plt.subplots(nrows=2, ncols=3, gridspec_kw=dict(hspace=0.3), figsize=(17, 8))
fig.tight_layout()

for ax, col in zip(axes.flatten(), df_num_cols.columns):
    sns.boxplot(x=df_num_cols[col], color='green', ax=ax)

# Remove the unused subplot if there are only 5 numeric variables
fig.delaxes(axes[1, 2])

# Optionally, add a title
# fig.suptitle('visualize and compare the distribution and central tendency of numerical attributes', color='k', fontsize=12)

plt.show()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

columns = df_num_cols.columns[: len(df_num_cols.columns)]
fig = plt.figure()
fig.set_size_inches(18, 8)
length = len(columns)
for i,j in itertools.zip_longest(columns, range(length)):
    plt.subplot((length // 2), 3, j+1)
    plt.subplots_adjust(wspace = 0.2, hspace = 0.5)
    df_num_cols[i].hist(bins = 20, edgecolor = 'black')
    plt.title(i)
# fig = fig.suptitle('distribution of numerical attributes', color = 'r' ,fontsize = 14)
plt.show()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
